{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Logistic Regression 1.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name suggests it's a regression but it is used for classification, why? Need to understand why linear regression will not work in a classification statement.\n",
    "\n",
    "- In linear regression, consider the above picture example. Say we have a best fit line that gives weight 75 and above as obese. But if there is an outlier point in obese at say 180, the best fit line will curve towards that point as linear regression is sensitive towards outliers. So now, instead of 75, maybe 90 or 95 only will be considered obese, which is wrong from the fact\n",
    "\n",
    "- In linear regression for classification, as seen above the values will not be limited between 0 and 1. It goes well beyond 1 and well less than 0. \n",
    "\n",
    "To overcome the above two problems, logistic regression is used in a classification problem statement. It uses sigmoid funtion to squash the line at 0 and 1. Logistic regression supports binary as well as multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression definition states that it can be used to a classification problem, if the two classes are linearly separable. But in interviews if they ask which model will you choose for a problem, just say you'll try multiple algorithmns and see which works better.\n",
    "\n",
    "Assumptions:?\n",
    "\n",
    "- the 'y'(target) value in +ve as 1, -ve as -1 \n",
    "- similar to linear regression, assume the best fit line passes to origin (0,0). The equation y=mx+c becomes y=mx as intercept is 0\n",
    "- y=w**t(x) (equation from above step 2 assumption) is nothing but the distance of the point from the best fit line\n",
    "- above the line the distance will be +ve and below the line the distance will be -ve\n",
    "\n",
    "Consider the cases where crt and wrong classification is done in the below picture\n",
    "\n",
    "<img src = 'Logistic Regression 2.PNG'/>\n",
    "\n",
    "The cost funtion (optimizer) should be as maximum as possible, to attain that the best fit line will keep on updating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does the logistic regression solves the outlier problem of linear regression? Consider the example below:\n",
    "\n",
    "<img src = 'Logistic Regression 3.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above picture, the vertical split up line gave the best classification, but the one outlier point at 500 distance didn't give the max value for the optimizer(cost funtion) it gave -480. The horizontal fit line gave the worst classification, but the cost function was positive at +2. So this is the problem of linear regression. \n",
    "\n",
    "However, the logistic regression solves this by enclosing the cost funtion inside a function called sigmoid function. Still the target is that the value of the optimizer should be max, but the cost function value is passed to the sigmoid function, which results the values between 0 and 1 in turn reducing the effects of the outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass (One vs Rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Logistic Regression 4.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each output class in a multiclass problem statement, there will be one model created by LR which treats that specific class as +ve and all other class as -ve. Likewise, if like above three classes are present, three models will be created. For a new test data, the independent features will be passed to all the three models. Each model will give a probability for its class. The sum of probabilities from all the models will be 1. The class with the highest probability is taken as the output from the model. To use this in code, set the parameter 'multiclass' as 'ovr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
