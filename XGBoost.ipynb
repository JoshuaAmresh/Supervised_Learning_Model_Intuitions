{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'XGBC_1.PNG'/>\n",
    "\n",
    "- Here, the base model is not talking about the output, but rather the probability of getting the right output\n",
    "- First residual is calculated by subbing probability from actuals\n",
    "- Now we construct the next tree sequentially, splitting based on one feature\n",
    "- Similarity weight is used to calculate the information gain and see which split is working in the tree\n",
    "- We can split by any feature. We take the split which is giving maximum information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'XGBC_2.PNG'/>\n",
    "\n",
    "- we try different splits to see which split is working with most gain\n",
    "- cover value is used to post prune the branches which is lesser than the cover value\n",
    "- here, the cover value is Pr(1-Pr) i.e. the denominator of similarity weight function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'XGBC_3.PNG'/>\n",
    "\n",
    "- now we calculate the output of a record, by following the tree path that we created\n",
    "- the final value is sigmoid(base model output+learning_rate*similarity_weight_of_result_branch\n",
    "- to calculate the base model output, which was giving only probability till now, we use log(odds)\n",
    "- then we apply sigmoid activation function to get the output 2nd probability and in turn calculate the second residual\n",
    "\n",
    "<img src = 'XGBC_4.PNG'/>\n",
    "\n",
    "- the lambda in the denominator of the similarity weight function is a hyper-parameter for tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'XGBR_1.PNG'/>\n",
    "\n",
    "- here the base model gives the average value as output\n",
    "- with that residual 1 is calculated\n",
    "- then we make a sequential decision tree and split it with a feature\n",
    "- again, we can split with any feature and any value inside a feature\n",
    "- the split with the best information gain is taken as the first split\n",
    "- for gain, similarity weight is again used, but in XGBRegressor there is a change in the denominator of the similarity weight function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'XGBR_2.PNG'/>\n",
    "\n",
    "- after deciding on a split, the output at a leaf node is the average of the values in that leaf\n",
    "- now to calculate the output for a record, we use formula : base_model_op+learning_rate*output_of_the_branch_record_ended_at\n",
    "- now this will give the second output, which will be used to calculate the second residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'XGBR_3.PNG'/>\n",
    "\n",
    "- now with the new residual we create another tree and so on we proceed till we get accurate and generalized results\n",
    "- the final value will be sum of base model and all the decision trees output calculated from start till end\n",
    "- the hyper-parameter here is the gamma for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
